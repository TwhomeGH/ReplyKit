//
//  SampleHandler.swift
//  ReplyKIT
//
//  Created by user on 2025/8/24.
//

import MediaPlayer
import VideoToolbox
import ReplayKit
import RTMPHaishinKit

import os

#if os(iOS)
import UIKit
#elseif os(macOS)
import AppKit

#endif

import HaishinKit
import AVFoundation    // æä¾› AVAudioPCMBuffer, AVAudioFormat, AVAudioTime ç­‰
import CoreAudio
import CoreMedia       // æä¾› CMSampleBuffer, CMSampleBufferGetFormatDescription ç­‰
import CoreImage


import os
import Foundation


let logger = Logger(subsystem: "nuclear.liveAPP.ReplyKit", category: "extension")
let userDefaults=UserDefaults(suiteName: "group.nuclear.liveAPP")



@available(iOS 10.0, *)
class SampleHandler: RPBroadcastSampleHandler , @unchecked Sendable{

    var DWidth = 1920
    var DHeight = 1334

    var audioProcessor: AudioProcessor!
    var videoProcessor: VideoFrameProcessor!

    var streamStataus:MyStreamBitRateStrategy!

    var volumeCheckTimer: Timer?

    var volumeNotifier : VolumeNotifier?

    var isVideoRotationEnabled = true


    

#if os(iOS)
    private var currentOrientation: UIDeviceOrientation = .portrait
    private var nowOrientation: UIDeviceOrientation = .landscapeLeft

#else
    private var currentOrientation: Int = 0
    private var nowOrientation:Int = 0

#endif


    // MARK: ç”¨æˆ¶è¨­ç½®è¼¸å‡ºå¯¬é«˜
    var ADWidth : Int
    var ADHeight : Int

    private var lastVideoOrientation: AVCaptureVideoOrientation?

    var base:Int = 100_000
    var multiplier:Int = 39
    // 100_000 * 30 = 3_000_000 bps
    var bitrate:Int {

        didSet {
            Task{

                guard let streamStataus = streamStataus else {
                    sendlog(message: "âš ï¸ streamStataus å°šæœªåˆå§‹åŒ–ï¼Œç„¡æ³•æ›´æ–° BitRate")
                    return
                }
                let VSet=await streamStataus.mamimumVideoBitRate

                sendlog(message: "Old BitRate:\(VSet)")



                await streamStataus.updateVideoBitRate(to: bitrate)

                sendlog(message: "New BitRate:\(VSet)")
            }
        }
    }






    // MARK: å…¨å±€ MediaMixer
    let mediaMixer:MediaMixer = MediaMixer(captureSessionMode: .manual, multiTrackAudioMixingEnabled: true)



    private var lastVideoTimestamp: CMTime = .zero





    private var onAudioPage = userDefaults?.bool(forKey: "onAudioPage") ?? false

    private var needVideoConfiguration = true
    private var needAudioConfiguration = true

    private var isSessionReady = false
    private var appVolume: Float = 1.0
    private var micVolume: Float = 1.0
    private var appAddVolume: Float = 1.0
    private var micAddVolume: Float = 1.0

    private var rtmpConnection = RTMPConnection()


    private var rtmpStream : RTMPStream!

    var videoBufferManager: AdaptiveVideoBufferManager?

    private var lastConfiguredSize: CGSize? = nil



    private func reloadVolumes(type:Int = -1,volume:Float = 1.0) {
        //sendlog(message:"app audio \(appVolume)\(micVolume)")

        switch type {
        case 0:
            appVolume = volume
            //sendlog(message:"app audio update\(appVolume)")
            break;
        case 1:
            micVolume = volume
            //sendlog(message:"mic audio update\(micVolume)")
            break;

        default:
            appVolume = userDefaults?.float(forKey: "appVolume") ?? 1.0
            micVolume = userDefaults?.float(forKey: "micVolume") ?? 1.0
            sendlog(message:"app mic audio update \(appVolume) \(micVolume)")

        }

    }

    //

    
    

    // MARK: è¨»å†Šæ‰€æœ‰äº‹ä»¶
    func registerObservers() {
        let center = CFNotificationCenterGetDarwinNotifyCenter()
        for event in Eventlisten.shared.eventNames {
            CFNotificationCenterAddObserver(center,
                                            UnsafeRawPointer(Unmanaged.passUnretained(self).toOpaque()),
                                            { (_, observer, name, _, _) in
                guard let observer = observer,
                      let cfName = name else { return }
                let handler = Unmanaged<SampleHandler>.fromOpaque(observer).takeUnretainedValue()
                let eventName = cfName.rawValue as String
                handler.handleEvent(eventName: eventName)
            },
                                            event as CFString,
                                            nil,
                                            .deliverImmediately)
        }
    }


    // MARK: ç§»é™¤æ‰€æœ‰è§€å¯Ÿè€…
    private func removeObservers() {
        let center = CFNotificationCenterGetDarwinNotifyCenter()
        for event in Eventlisten.shared.eventNames {
            CFNotificationCenterRemoveObserver(center,
                                               UnsafeRawPointer(Unmanaged.passUnretained(self).toOpaque()),
                                               CFNotificationName(event as CFString),
                                               nil)
        }
    }

    // MARK: çµ±ä¸€è™•ç†äº‹ä»¶
    func handleEvent(eventName: String) {
        switch eventName {
        case "micAdd":

            let newVolume = userDefaults?.double(forKey: "micAddVoulme") ?? 1.0
            micAddVolume=Float(newVolume)
            guard let audioProcessor else { return }

            Task {
                audioProcessor.updateVolumes(micAdd: micAddVolume)
            }
            sendlog(message: String(
                format: "éº¥å…‹é¢¨éŸ³é‡æ”¾å¤§: %.5f%%",
                newVolume
            ))
        case "appAdd":

            let newVolume = userDefaults?.double(forKey: "appAddVoulme") ?? 1.0
            appAddVolume=Float(newVolume)
            guard let audioProcessor else { return }
            Task {
                audioProcessor.updateVolumes(appAdd: appAddVolume)
            }
            sendlog(message: String(
                format: "AppéŸ³é‡æ”¾å¤§: %.5f%%",
                newVolume
            ))


        case "micVolumeChanged":
            
            let newVolume = userDefaults?.double(forKey: "micVolume") ?? 1.0
            micVolume=Float(newVolume)

            guard let audioProcessor else { return }
            Task {
                audioProcessor.updateVolumes(mic: micVolume)
            }
            sendlog(message: String(
                format: "éº¥å…‹é¢¨éŸ³é‡æ›´æ–°: %.2f%% (åŸå§‹å€¼: %.5f)",
                volumeToPercentage(newVolume),
                newVolume
            ))
            Task { await updateMicAudioVolume(Float(newVolume)) }

        case "appVolumeChanged":
            let newVolume = userDefaults?.double(forKey: "appVolume") ?? 1.0
            appVolume=Float(newVolume)
            guard let audioProcessor else { return }
            Task {
                audioProcessor.updateVolumes(app: appVolume)
            }

            sendlog(message: String(
                format: "!!æ‡‰ç”¨éŸ³é‡æ›´æ–°: %.2f%% (åŸå§‹å€¼: %.5f)",
                volumeToPercentage(newVolume),
                newVolume
            ))

            Task { await updateAppAudioVolume(Float(newVolume)) }
            
        case "orientationChanged":
#if os(iOS)
            if let orientationValue = userDefaults?.integer(forKey: "Orientation"),


                let orientation = UIDeviceOrientation(rawValue: orientationValue) {

                sendlog(message: "OO:\(orientationValue) \(orientation)")
                Task {
                    configureOrientation()
                }


            }
#else
            print("No Make tihs!")

#endif

        case "videoRotateChanged":
            sendlog(message: "æ£„ç”¨æ–¹æ³•ï¼")
            break
            //isVideoRotationEnabled=userDefaults?.bool(forKey: "VideoRotate") ?? true

//            Task {
//                videoProcessor.updateRotator(status: rotator)
//            }
            //logger.info("AutoVideoRotate:\(self.isVideoRotationEnabled)")


        case "DebugRotate":
            let Rlog=userDefaults?.bool(forKey: "EnableRotatelog") ?? false
            videoProcessor.rotator?.debug = Rlog
            sendlog(message:"[æ—‹è½‰æ—¥èªŒè®ŠåŒ–] VideoRotate \(Rlog)")


        case "useBic":
            let Rlog=userDefaults?.bool(forKey: "useBic") ?? true
            videoProcessor.rotator?.useBic = Rlog
            sendlog(message:"[GPU ä½¿ç”¨Bicè™•ç†] \(Rlog)")


        case "bitRateChange":
            sendlog(message: "NewBit: \(bitrate)")

            bitrate=userDefaults?.integer(forKey: "bitRate") ?? 3_900_000


        case "logURL":
            let logM=userDefaults?.string(forKey: "logURL") ?? "http://192.168.0.242/post"
            RPConfig.shared.logURL = logM
            sendlog(message: "LOG URL: \(logM)")


        case "logMode":
            let logM=userDefaults?.integer(forKey: "logMode") ?? 0
            sendlog(message: "LOG Mode \(logM)")
            if logM == 0 {
                LogManager.shared.forceFlush()
            }
            RPConfig.shared.logMode=logM


        case "onlogPage":
            let logPage=userDefaults?.bool(forKey: "onlogPage") ?? false

            RPConfig.shared.onLogPage=logPage
            if logPage {

                videoBufferManager?.adjustInterval = 3.0
                LogManager.shared.flushInterval = 1.0

                // å…ˆå–æ¶ˆèˆŠçš„ timerï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if let oldTimer = LogManager.shared.flushTimer {
                    oldTimer.cancel()
                    LogManager.shared.flushTimer = nil
                }

                LogManager.shared.setupFlushTimer()

                LogManager.shared.notifyThrottle = 1.0
                sendlog(message: "æ­£åœ¨LOG NTime:\(LogManager.shared.notifyThrottle)")
            } else {

                videoBufferManager?.adjustInterval = 30.0

                // å…ˆå–æ¶ˆèˆŠçš„ timerï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if let oldTimer = LogManager.shared.flushTimer {
                    oldTimer.cancel()
                    LogManager.shared.flushTimer = nil
                }


                LogManager.shared.flushInterval = 10.0
                LogManager.shared.setupFlushTimer()


                LogManager.shared.notifyThrottle = 20.0
                sendlog(message: "éLOG NTime:\(LogManager.shared.notifyThrottle)")
            }


        case "OutW":
            let dstRW=userDefaults?.integer(forKey: "dstW") ?? 0

            ADWidth = dstRW
            videoProcessor.rotator?.dstWW = dstRW
            sendlog(message: "OutW:\(dstRW)")


        case "OutH":
            let dstRH=userDefaults?.integer(forKey: "dstH") ?? 0
            ADHeight = dstRH
            videoProcessor.rotator?.dstHH = dstRH

            sendlog(message: "OutW:\(dstRH)")




        case "Enablelog":
            let Enablelog=userDefaults?.bool(forKey: "Enablelog") ?? false
            sendlog(message: "é–‹é—œæ—¥èªŒlog")
            RPConfig.shared.enableLog=Enablelog


        case "onAudioPage":
            onAudioPage=userDefaults?.bool(forKey: "onAudioPage") ?? false



                if audioProcessor != nil {

                    audioProcessor.updatePage(status: onAudioPage)
                    sendlog(message:"[Audio] Page \(onAudioPage)")

                }
                
                else {
                    let onPause=userDefaults?.bool(forKey: "PauseStream") ?? false

                    if onPause {
                        sendlog(message: "æ­£åœ¨æš«åœ å–æ¶ˆé‡å»ºAudio")
                        return
                    }

                    sendlog(message:"[Audio] audioProcessor is nil Rebuild AudioProcessor!")
                    audioProcessor = AudioProcessor(
                        mediaMixer: mediaMixer,
                        volumeNotifier: volumeNotifier!,
                        appAddVolume: appAddVolume,
                        micAddVolume: micAddVolume,
                        appVolume: appVolume,
                        micVolume: micVolume,
                        onAudioPage: onAudioPage
                    )
                
                    // å‡è¨­ AudioProcessor æœ‰ç„¡åƒæ•¸çš„åˆå§‹åŒ–æ–¹æ³•
                        audioProcessor?.updatePage(status: onAudioPage)


                }



            sendlog(message: "AudioPage:\(onAudioPage)")


        case "PauseStream":
            self.broadcastPaused()
            sendlog(message: "ä½ æš«åœç›´æ’­ç•«é¢ï¼")
        case "ResumeStream":
            self.broadcastResumed()
            sendlog(message: "ä½ ç°å¾©äº†ç›´æ’­ç•«é¢ï¼")
            

            
        default:
            break
        }
    }

    deinit {

        rtmpStream = nil

        removeObservers()
    }

    // MARK: åˆå§‹åŒ–
    override init() {

        let saved = UserDefaults.standard.integer(forKey: "bitRate")

        bitrate =  saved != 0 ? saved : base * multiplier


        rtmpStream = RTMPStream(connection: rtmpConnection)

        ADWidth = 0
        ADHeight = 0

        super.init()


        registerObservers()
        logger.info("ReplyKit Debug")




        Task {
            // åˆå§‹è®€å–
            reloadVolumes()
        }
    }





    func updateAppAudioVolume(_ volume: Float) async {
        var settings = await mediaMixer.audioMixerSettings
        if var track = settings.tracks[0] {   // 0 æ˜¯ app éŸ³é » track
            track.volume = volume            // volume å€¼ 0.0 ~ 1.0
            settings.tracks[0] = track
        }
        await mediaMixer.setAudioMixerSettings(settings)
    }

    func updateMicAudioVolume(_ volume: Float) async {
        var settings = await mediaMixer.audioMixerSettings
        if var track = settings.tracks[1] {   // 1 æ˜¯éº¥å…‹é¢¨ track
            track.volume = volume
            settings.tracks[1] = track
        }
        await mediaMixer.setAudioMixerSettings(settings)
    }

#if os(iOS)
    // --- å°‡åŸä¾†çš„ updateVideoOrientation æ”¹æˆä¸‹é¢é€™å€‹ ---
    func avOrientation(from deviceOrientation: UIDeviceOrientation) -> AVCaptureVideoOrientation? {
        switch deviceOrientation {
        case .portrait:
            return .portrait
        case .portraitUpsideDown:
            return .portraitUpsideDown
        case .landscapeLeft:
            // æ³¨æ„ï¼šUIDevice.landscapeLeft è¡¨ç¤ºè£ç½®å·¦é‚Šæœä¸‹ï¼Œå° camera æ–¹å‘å¯èƒ½è¦åå‘æ˜ å°„è¦–é¡é ­è€Œå®š

            return .landscapeLeft
        case .landscapeRight:
            return .landscapeRight
        default:
            return .landscapeLeft
        }
    }

    func isLandscape(_ orientation: AVCaptureVideoOrientation) -> Bool {
        switch orientation {
        case .landscapeLeft, .landscapeRight:
            return true
        default:
            return false
        }
    }

    func updateVideoOrientation(from orientation: UIDeviceOrientation) async {
        // è½‰æˆ AVFoundation çš„æ–¹å‘
        guard let avOrientation = avOrientation(from: orientation) else { return }

        var videoSettings = await rtmpStream.videoSettings
        let size = videoSettings.videoSize
        let newSize:CGSize

        switch avOrientation {
        case .portrait, .portraitUpsideDown:
            newSize = CGSize(width: size.height, height: size.width)
            sendlog(message: "ç›´å‘:\(size) â†’ \(videoSettings)")

        default:
            newSize = CGSize(width: size.height, height: size.width)
            sendlog(message: "æ©«å‘:\(size) â†’ \(videoSettings)")


            break;
        }

        // å¦‚æœä¸Šæ¬¡å·²ç¶“è¨­éé€™å€‹ avOrientationï¼Œå°±ä¸ç”¨å†è¨­
        guard avOrientation != lastVideoOrientation || videoSettings.videoSize != newSize else { return }
        lastVideoOrientation = avOrientation




        videoSettings.videoSize = newSize
        try? await rtmpStream.setVideoSettings(videoSettings)


        await mediaMixer.setVideoOrientation(avOrientation)
        sendlog(message: "æ›´æ–°æ–¹å‘: \(orientation) -> \(avOrientation)")
        sendlog(message: "Size:\(newSize) - \(videoSettings)")
    }

#endif





    func configureOrientation() {
        let manager = DeviceOrientationManager.shared   // ä½¿ç”¨å–®ä¾‹
        let lockedValue = userDefaults?.bool(forKey: "LockIN") ?? false
        if  lockedValue {
            sendlog(message:"\(lockedValue)ä¸åµæ¸¬ åˆå§‹åŒ–ä¸€æ¬¡")
            manager.isEnabled = false
            manager.stopUpdates()
        } else {
            // è§£é–æ–¹å‘ â†’ å•Ÿå‹• Motion åµæ¸¬
            manager.isEnabled = true
            sendlog(message:"åµæ¸¬é–‹å•Ÿ")
            manager.startUpdates()
            manager.orientationChanged = { [weak self] deviceOrientation in

                sendlog(message: "æ–¹å‘Freeä¸­")
                #if os(iOS)
                guard let self else { return }
                Task.detached(priority: .utility) {

                    await self.updateVideoOrientation(from: deviceOrientation)
                }
                #endif

            }
        }
    }

    var isStopping = false

    func stopBroadcastWithError(_ message: String) {

        guard !isStopping else { return }
        isStopping = true

        let error = NSError(domain: "com.liveApp.broadcast",
                            code: -1,
                            userInfo: [NSLocalizedDescriptionKey: message])
        // å¦‚æœ broadcastEnd æ˜¯ async
           Task {
               broadcastEnd(message: message)  // ç­‰å¾…æ¸…ç†å®Œæˆ
               await MainActor.run {
                   finishBroadcastWithError(error)

               }
           }

    }





    private var disconnectMonitorTask: Task<Void, Never>?

    // MARK: æ–·ç·šæª¢æ¸¬
    func startDisconnectMonitor() {
        disconnectMonitorTask = Task.detached { [weak self, weak streamStataus] in
            while !(self?.isStopping ?? true) {
                try? await Task.sleep(nanoseconds: 1_000_000_000)
                await streamStataus?.checkDisconnect(timeout: 5)
            }
        }
    }


    func prepareCompressionSession(){
        var compressionSession: VTCompressionSession?

        let status = VTCompressionSessionCreate(
            allocator: kCFAllocatorDefault,
            width: 1920,
            height: 1080,
            codecType: kCMVideoCodecType_H264,
            encoderSpecification: nil,
            imageBufferAttributes: nil,
            compressedDataAllocator: nil,
            outputCallback: nil,
            refcon: nil,
            compressionSessionOut: &compressionSession
        )

        if status == noErr {
            logger.info("âœ… å»ºç«‹æˆåŠŸ: \(String(describing: compressionSession))")
        } else {
            logger.info("âŒ å»ºç«‹å¤±æ•—: \(status)")
        }
        if let session = compressionSession {
            var supportedProps: CFDictionary?
            if VTSessionCopySupportedPropertyDictionary(session, supportedPropertyDictionaryOut: &supportedProps) == noErr {

                if let props = supportedProps as? [String: Any] {
                            logger.debug("âœ… Supported properties: \(props)")
                        } else {
                            logger.debug("âš ï¸ ç„¡æ³•è½‰æ› CFDictionary")
                        }


            } else {
                logger.debug("âŒ ç„¡æ³•å–å¾— SupportedPropertyDictionary")
            }


        }
        compressionSession = nil

    }



    func setUserDefalutConfig(urlString:String,streamKey:String)  {
        isVideoRotationEnabled = userDefaults?.bool(forKey: "VideoRotate") ?? true

        isStopping = false

        if userDefaults?.object(forKey: "appVolume") == nil {
            userDefaults?.set(1.0, forKey: "appVolume")
        }
        if userDefaults?.object(forKey: "micVolume") == nil {
            userDefaults?.set(1.0, forKey: "micVolume")
        }

        // MARK: Video dimensions
        ADWidth = userDefaults?.integer(forKey: "dstW") ?? 0
        ADHeight = userDefaults?.integer(forKey: "dstH") ?? 0

        if ADWidth > 0 && ADHeight > 0 {
            DWidth = ADWidth
            DHeight = ADHeight
        }


        bitrate=userDefaults?.integer(forKey: "bitRate") ?? 3_900_000


        // MARK: Volume
        let newMicAddVolume = userDefaults?.double(forKey: "micAddVoulme") ?? 1.0
        let newAppAddVolume = userDefaults?.double(forKey: "appAddVoulme") ?? 1.0

        micAddVolume=Float(newMicAddVolume)
        appAddVolume=Float(newAppAddVolume)



        // çµ„æˆå®Œæ•´ RTMP URL
        let fullURLString = "\(urlString)/\(streamKey)"


        // MARK: æ˜¯å¦åœ¨æ—¥èªŒLog mode
        RPConfig.shared.logMode = userDefaults?.integer(forKey: "logMode") ?? 0
        RPConfig.shared.onLogPage = userDefaults?.bool(forKey: "onlogPage") ?? false


        // ğŸ”¹ è½‰æˆ URL
        sendlog(message: "ğŸ”¹ æ¨æµ URL:\(fullURLString)")
        sendlog(message: "App:\(appVolume)  Mic:\(micVolume) AppAdd:\(appAddVolume) MicAdd:\(micAddVolume)")



        reloadVolumes()

    }


    func configureVideo() async {
        // Video settings
        var videoSettings = await rtmpStream.videoSettings
        videoSettings.scalingMode = .letterbox
        videoSettings.profileLevel = kVTProfileLevel_H264_High_AutoLevel as String
        videoSettings.videoSize = .init(width: 1334, height: 1920)
        videoSettings.maxKeyFrameIntervalDuration = 2
        try? await rtmpStream.setVideoSettings(videoSettings)

        // Video mixer passthrough
        var videoMixerSettings = await mediaMixer.videoMixerSettings
        videoMixerSettings.mode = .passthrough


        await mediaMixer.setVideoMixerSettings(videoMixerSettings)


        // ReplayKit is sensitive to memory, so we limit the queue to a maximum of five items.
        await rtmpStream.setVideoInputBufferCounts(5)


    }
    func configureAudio() async {
        // Audio settings
        var audioSettings = await mediaMixer.audioMixerSettings
        audioSettings.tracks[0] = .default
        audioSettings.tracks[1] = .default


        await mediaMixer.setAudioMixerSettings(audioSettings)



    }
    // MARK: Video Setting
    func configureMediaMixer() async {

        streamStataus = MyStreamBitRateStrategy()

        await streamStataus.refreshStatusTimestamp()

        await streamStataus.setOnDisconnect { [weak self] in
            self?.stopBroadcastWithError("RTMP æ–·ç·š")
        }

        await rtmpStream.setBitRateStrategy(streamStataus)


        await mediaMixer.addOutput(rtmpStream)
        await mediaMixer.startRunning()



        configureOrientation()


//        #if os(iOS)
//
//        let videofrom = await UIDevice.current.orientation
//        await updateVideoOrientation(from: videofrom)
//
//        #endif

    }


   // MARK: Process

    func initProcessors() async {
        videoBufferManager = AdaptiveVideoBufferManager()
        volumeNotifier = VolumeNotifier()


            videoProcessor = VideoFrameProcessor(
                mediaMixer: mediaMixer,
                videoBufferManager: videoBufferManager!,
                rtmpStream: rtmpStream,
                sendlog: { message in
                    sendlog(message: message)
                }
            )


            audioProcessor = AudioProcessor(
                mediaMixer: mediaMixer,
                volumeNotifier: volumeNotifier!,
                appAddVolume: appAddVolume,
                micAddVolume: micAddVolume,
                appVolume: appVolume,
                micVolume: micVolume,
                onAudioPage: onAudioPage
            )




    }


    func startRTMP(url:String,key:String) async {
        do {

            // step 3: é€£ç·š RTMP

            _ = try await rtmpConnection.connect(url)

            _ = try await rtmpStream.publish(key)

            // step 4: æ¨™è¨˜ session ready
            await MainActor.run {
                // Add output
                self.isSessionReady = true
                logger.info("ğŸ‰ RTMP æ¨æµæˆåŠŸ")


            }

        }  catch RTMPConnection.Error.requestFailed(let response) {
            self.stopBroadcastWithError("RTMP æœå‹™å™¨é€£ç·šå¤±æ•— \(response)")

        }  catch RTMPStream.Error.requestFailed(let response) {
            self.stopBroadcastWithError("RTMP æ¨æµå¤±æ•— \(response)")

        } catch {
        self.stopBroadcastWithError("RTMP å…¶ä»–éŒ¯èª¤ \(error)")

    }


    }
    // MARK: ç›´æ’­é–‹å§‹
    override func broadcastStarted(withSetupInfo setupInfo: [String : NSObject]?) {
        // User has requested to start the broadcast. Setup info from the UI extension can be suppdlied but optional.

        logger.info("é‹è¡Œé€šçŸ¥")



        // ğŸ”¹ å¾ UserDefaults æ‹¿ RTMP è¨­å®š
        let urlString = userDefaults?.string(forKey: "rtmpURL") ?? "rtmp://192.168.0.102/live"
        let streamKey = userDefaults?.string(forKey: "rtmpKey") ?? "stream1?vhost=live2"

        setUserDefalutConfig(
            urlString: urlString,
            streamKey: streamKey
        )



        self.prepareCompressionSession()

        Task {

            await configureVideo()
            await configureAudio()
            await configureMediaMixer()

             logger.info("âœ… MediaMixer é…ç½®å®Œæˆ")

            await initProcessors()

             logger.info("âœ… Processor åˆå§‹åŒ–å®Œæˆ")




            await startRTMP(url: urlString , key: streamKey)



        }

    }


    // MARK: - æš«åœç•«é¢æ§åˆ¶
    private var pauseTimer: DispatchSourceTimer?

    // MARK: é‡ç”¨æš«åœ
    private var pausedNV12PixelBuffer: CVPixelBuffer?
    private var pausedBGRAcontext: CGContext?
    private var pausedBGRABuffer: CVPixelBuffer?

    var isPause = false


    private let stateQueue = DispatchQueue(label: "broadcast.state.queue")

    private var pausedFrameTimestamp: CMTime = .zero
    private let pausedFrameDuration = CMTimeMake(value: 1, timescale: 1) // æ¯ç§’ä¸€å¹€

    private var pausedStartTime = CACurrentMediaTime()
    private var pausedFrameIndex: Int = 0

    // MARK: - æš«åœç•«é¢é‚è¼¯
    private func startPausedFrameLoop() {
        let width = DWidth
        let height = DHeight

        // ç›´æ¥å»ºç«‹æš«åœç•«é¢è³‡æº
        if pausedBGRABuffer == nil {
            pausedBGRABuffer = createPixelBuffer(width: width, height: height,
                                                 format: kCVPixelFormatType_32BGRA, reuse: nil)
            if let bgra = pausedBGRABuffer {
                pausedBGRAcontext = createContext(for: bgra)
                if let ctx = pausedBGRAcontext {
                    updatePausedContext(buffer: bgra, context: ctx, text: "ç›´æ’­æš«åœä¸­")
                    sendlog(message: "âœ… æš«åœç•«é¢ BGRA buffer å»ºç«‹æˆåŠŸ")
                }
            }
        }

        if pausedNV12PixelBuffer == nil {
            pausedNV12PixelBuffer = createPixelBuffer(width: width, height: height,
                                                      format: kCVPixelFormatType_420YpCbCr8BiPlanarFullRange, reuse: nil)
        }

        // å•Ÿå‹•å®šæ™‚å™¨æ¨å¹€
        pauseTimer?.cancel()
        pauseTimer = DispatchSource.makeTimerSource(queue: .global(qos: .utility))
        pauseTimer?.schedule(deadline: .now(), repeating: 1.0 / 30.0)
        pauseTimer?.setEventHandler { [weak self] in
            guard let self = self,
                  let bgra = self.pausedBGRABuffer,
                  let nv12 = self.pausedNV12PixelBuffer else { return }

            convertBGRAtoNV12(bgra: bgra, nv12: nv12)

            var frameIndex = 0
            self.stateQueue.sync { frameIndex = self.pausedFrameIndex }

            if let sampleBuffer = createSampleBuffer(from: nv12, frameIndex: &frameIndex) {
                self.stateQueue.sync { self.pausedFrameIndex = frameIndex }
                Task { await self.mediaMixer.append(sampleBuffer) }
            }
        }
        pauseTimer?.resume()
    }


    // MARK: ç›´æ’­æš«åœ
    override func broadcastPaused() {
        stateQueue.sync {
            guard !isPause else {
                sendlog(message: "âš ï¸ å·²è™•æ–¼æš«åœç‹€æ…‹ï¼ˆé˜²é‡è¤‡è§¸ç™¼ï¼‰")
                return
            }
            isPause = true
            pausedFrameIndex = 0
            pausedStartTime = CACurrentMediaTime()
            pausedFrameTimestamp = .zero
            sendlog(title: "SampleHandler", message: "âš ï¸ Broadcast paused - sending paused frame repeatedly")
        }

        // åœæ­¢ Audio / Video è™•ç†
        audioProcessor?.cleanup()
        videoProcessor?.cleanup()
        volumeNotifier?.cleanup()

        audioProcessor = nil
        videoProcessor = nil
        volumeNotifier = nil

        // MARK: === å»ºç«‹æš«åœç•«é¢è³‡æº ===
        // å‘¼å«å°ˆé–€è™•ç†æš«åœç•«é¢é‚è¼¯
        startPausedFrameLoop()


    }

    // MARK: ç›´æ’­æ¢å¾©
    override func broadcastResumed() {
        stateQueue.sync {
            guard isPause else {
                sendlog(message: "âš ï¸ éæš«åœç‹€æ…‹ï¼Œå¿½ç•¥æ¢å¾©æ“ä½œï¼ˆé˜²é‡è¤‡è§¸ç™¼ï¼‰")
                return
            }
            isPause = false
        }


        sendlog(title: "SampleHandler", message: "ğŸ¬ Broadcast resumed - stopping paused frame timer")

        // åœæ­¢æš«åœç•«é¢å®šæ™‚å™¨
        if let timer = pauseTimer {
            timer.cancel()
            pauseTimer = nil
            sendlog(message: "ğŸ›‘ å·²åœæ­¢æš«åœç•«é¢å®šæ™‚å™¨")
        }

        // æ¸…ç†æš«åœç•«é¢è³‡æº
        pausedBGRABuffer = nil
        pausedBGRAcontext = nil
        pausedNV12PixelBuffer = nil

        // é‡å»ºæˆ–å•Ÿç”¨éŸ³é‡ç›£è½å™¨
        if volumeNotifier == nil {
            volumeNotifier = VolumeNotifier()
            sendlog(message: "ğŸ”Š VolumeNotifier é‡æ–°å»ºç«‹")
        }

        // MARK: é‡å»º VideoProcessor
        if videoProcessor == nil {
            videoProcessor = VideoFrameProcessor(
                mediaMixer: mediaMixer,
                videoBufferManager: videoBufferManager!,
                rtmpStream: rtmpStream,
                sendlog: { message in
                    sendlog(message: message)
                }
            )
            sendlog(message: "ğŸ¥ VideoProcessor é‡å»ºå®Œæˆ")
        }

        // MARK: é‡å»º AudioProcessor
        if audioProcessor == nil {
            audioProcessor = AudioProcessor(
                mediaMixer: mediaMixer,
                volumeNotifier: volumeNotifier!,
                appAddVolume: appAddVolume,
                micAddVolume: micAddVolume,
                appVolume: appVolume,
                micVolume: micVolume,
                onAudioPage: onAudioPage
            )
            sendlog(message: "ğŸ§ AudioProcessor é‡å»ºå®Œæˆ")
        }

        // é‡æ–°å•Ÿç”¨éŸ³è¦–é »è™•ç†
        videoProcessor?.isActive = true
        audioProcessor?.isActive = true
        sendlog(message: "âœ… å·²é‡æ–°å•Ÿç”¨éŸ³è¦–é »è™•ç†")
    }

    
    // MARK: ç›´æ’­çµæŸè™•ç†
    func broadcastEnd(message:String = "æ­£å¸¸çµæŸ")  {
        // User has requested to finish the broadcast.

        // åœæ­¢æ–·ç·šç›£æ§ Task
        disconnectMonitorTask?.cancel()
        disconnectMonitorTask = nil

        needVideoConfiguration = true
        needAudioConfiguration = true


        removeObservers()


        isSessionReady = false

        Task {
#if os(iOS)
            await UIDevice.current.endGeneratingDeviceOrientationNotifications()

            NotificationCenter.default.removeObserver(self, name: UIDevice.orientationDidChangeNotification, object: nil)

#endif
        }

        DeviceOrientationManager.shared.stopUpdates()

        volumeNotifier?.cleanup()
        videoProcessor?.cleanup()
        audioProcessor?.cleanup()

        videoBufferManager = nil
        volumeNotifier=nil
        videoProcessor=nil
        audioProcessor=nil

        didConfigureVideo = false
        didConfigureAudio = false

        Task {

            await mediaMixer.removeOutput(rtmpStream)
            await mediaMixer.stopRunning()


            _ = try? await rtmpStream.close()
            _ = try? await rtmpConnection.close()
        }


        sendlog(message:"[RTMP] \(message)")
        LogManager.shared.forceFlush()


    }


    override func broadcastFinished() {


        broadcastEnd()

    }





    // MARK: å…§éƒ¨å·²é…ç½®è™•ç†
    private var didConfigureVideo = false

    private var didConfigureAudio = false

    func configureVideo(_ sampleBuffer: CMSampleBuffer) async {

        // å¦‚æœå·²ç¶“åˆå§‹åŒ–éï¼Œå°±ä¸å†é‡åš
        if didConfigureVideo { return }
        didConfigureVideo = true  // æ‰“ä¸Šæ¨™è¨˜



        let h264level = userDefaults?.string(forKey: "h264level")

        guard let formatDesc = sampleBuffer.formatDescription else { return }
        let dims = CMVideoFormatDescriptionGetDimensions(formatDesc)

        guard dims.width > 0 && dims.height > 0 else { return }

        var width = Int(dims.width)
        var height = Int(dims.height)

        if ADWidth > 0 && ADHeight > 0 {
            sendlog(message: "ç”¨æˆ¶è¨­å®šå¯¬é«˜ï¼š\(ADWidth) x \(ADHeight)")
            width = ADHeight
            height = ADWidth
        }

        if let orientationValue = CMGetAttachment(sampleBuffer, key: RPVideoSampleOrientationKey as CFString, attachmentModeOut: nil) as? NSNumber {
            sendlog(message: "ReplayKit ç•¶å‰ç•«é¢æ–¹å‘: \(orientationValue)")
        }

        let avfrom = lastVideoOrientation
        let newSize: CGSize

        switch avfrom {
        case .portrait, .portraitUpsideDown:
            newSize = CGSize(width: CGFloat(width), height: CGFloat(height))
            sendlog(message: "åˆå§‹æ›´æ–°ç›´å‘")
            await mediaMixer.setVideoOrientation(.portrait)
        default:
            newSize = CGSize(width: CGFloat(height), height: CGFloat(width))
            sendlog(message: "åˆå§‹æ›´æ–°æ©«å‘")
            await mediaMixer.setVideoOrientation(.landscapeRight)
            let bb = await mediaMixer.videoOrientation
            let b2 = await mediaMixer.videoInputFormats
            sendlog(message: "\(bb).\(b2)")
        }

        var videoSettings = await rtmpStream.videoSettings
        videoSettings.videoSize = newSize

        let profilelvl: String
        switch h264level {
        case "Baseline": profilelvl = kVTProfileLevel_H264_Baseline_AutoLevel as String
        case "Main": profilelvl = kVTProfileLevel_H264_Main_AutoLevel as String
        case "High": profilelvl = kVTProfileLevel_H264_High_AutoLevel as String
        case "ConstrainedBaseline": profilelvl = kVTProfileLevel_H264_ConstrainedBaseline_AutoLevel as String
        case "ConstrainedHigh": profilelvl = kVTProfileLevel_H264_ConstrainedHigh_AutoLevel as String
        case "Extended": profilelvl = kVTProfileLevel_H264_Extended_AutoLevel as String
        default: profilelvl = kVTProfileLevel_H264_Main_AutoLevel as String
        }

        sendlog(message: "H264Profilelevel: \(profilelvl)")
        videoSettings.profileLevel = profilelvl
        videoSettings.maxKeyFrameIntervalDuration = 2

        if lastConfiguredSize != newSize {
            try? await rtmpStream.setVideoSettings(videoSettings)
        }

        lastConfiguredSize = newSize
        DWidth = Int(newSize.width)
        DHeight = Int(newSize.height)

        sendlog(message: "æœ‰æ•ˆæ›´æ–°: \(newSize)")
        sendlog(message: "Video: \(videoSettings)")
        sendlog(message: "Video æ‹¿åˆ°ç•«é¢ \(width)x\(height)")
    }

    var lastlogTime : Double = 0.0
    var lastlogTimeAudio : Double = 0.0
    var logInterval : CFTimeInterval = 1.0

    override func processSampleBuffer(_ sampleBuffer: CMSampleBuffer, with sampleBufferType: RPSampleBufferType) {

//        guard isSessionReady else { return }

        switch sampleBufferType {
        case RPSampleBufferType.video:

            if needVideoConfiguration && !didConfigureVideo {
                needVideoConfiguration = false

                Task { [weak self] in
                        guard let self else { return }
                        await self.configureVideo(sampleBuffer)
                    }


                // âœ… åˆå§‹åŒ–æ™‚æ‰æŠ“ä¸€æ¬¡æ–¹å‘
#if os(iOS)
                if DeviceOrientationManager.shared.isEnabled {


                        let orientation = UIDevice.current.orientation
                        self.nowOrientation = orientation

                }
#endif

            }

            let timestamp = CMSampleBufferGetPresentationTimeStamp(sampleBuffer)

            lastVideoTimestamp = timestamp



            if videoProcessor != nil {
                videoProcessor.process(sampleBuffer, timestamp: timestamp)
            } else {
                if lastVideoTimestamp.seconds > lastlogTime + logInterval  {
                    sendlog(message: "Videoé€²ç¨‹ä¸å­˜åœ¨ï¼")
                    lastlogTime = lastVideoTimestamp.seconds
                }
            }



            break



        case RPSampleBufferType.audioApp, RPSampleBufferType.audioMic:
            if sampleBuffer.dataReadiness == .ready {
                let trackType: AudioTrackType = (sampleBufferType == .audioApp) ? .app : .mic

                if needAudioConfiguration  && !didConfigureAudio {
                    didConfigureAudio = true
                    needAudioConfiguration = false

                    let BitAudio=pcmBitrate(from: sampleBuffer)
                    let HHZ=BitAudio["HZ"] as? Int ?? 0
                    let CHH=BitAudio["Channel"] as? Int ?? 0
                    let BitR=BitAudio["BitRate"] as? Int ?? 0

                    // æ ¼å¼åŒ–å­—ä¸²
                    let logMessage = String(format: "SampleRate: %.0f Hz | Channels: %.0f | BitRate: %.1f kbps | BitO: %f",
                                            HHZ,
                                            CHH,
                                            BitR / 1000,
                                            BitR)

                    if let streamStatus = streamStataus {
                        Task {
                            await streamStatus.updateAudioBitRate(to: BitR)
                        }
                    }
                        sendlog(message: logMessage)

                }

                let timestamp = CMSampleBufferGetPresentationTimeStamp(sampleBuffer)

                lastVideoTimestamp = timestamp

                if audioProcessor != nil {
                    audioProcessor
                        .enqueue(sampleBuffer, trackType: trackType)
                } else {
                    if lastVideoTimestamp.seconds > lastlogTimeAudio + logInterval  {
                        sendlog(message: "Audioé€²ç¨‹ä¸å­˜åœ¨ï¼")
                        lastlogTimeAudio = lastVideoTimestamp.seconds
                    }
                }

            }
            break


        @unknown default:

            // Handle other sample buffer types
            fatalError("Unknown type of sample buffer")

        }
    }

}
